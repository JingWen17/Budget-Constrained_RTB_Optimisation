import pandas as pd
import numpy as np
from tqdm import tqdm
import random
from collections import defaultdict
import copy
import matplotlib.pyplot as plt

class Ready:
    """
    Get ready and start with first episode by randomly sample data and with random policy
    """
    def __init__(self, action_space: list, epsilon: float, **kwargs):
        """
        :param action_space:
        :param epsilon:
        :param kwargs:
        """
        self.action_space = action_space
        self.epsilon = epsilon

        self.q_table = defaultdict(lambda: 0)
        self.counts = defaultdict(lambda: 0)

    def rdm_pi(self):
        rdm_action = random.choice(self.action_space)
        return rdm_action


class generate_obs(Ready):
    """
    Generate observations
    """
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def get_samples(self,path):
        sandbox = pd.read_csv(path)
        self.total = len(sandbox)
        idx = random.randint(0,self.total-1)
        self.topics = sandbox['topic_id']
        self.clicks = sandbox['click']
        self.sampled_topic = int(self.topics[idx])
        self.sampled_click = int(self.clicks[idx])

        return self.sampled_topic, self.sampled_click

    def get_rewards(self,action,click):
        if action == self.action_space[1] and click == 1:
            reward = 5
            cost = 1.5
        else:
            reward = 0
            cost = 0

        return reward, cost

class MC_agent(Ready):

    def __init__(self, **kwargs):
        super().__init__(**kwargs)


    def learn_q(self,trajectory):
        """
        :param trajectory: list of [topic,left_money,action,reward]
        :return:
        """
        updated_return = {}
        episodic_counts = {}

        rewards = []
        for i in range(len(trajectory)):
            rewards.append(trajectory[i][3])

        #print(rewards, len(rewards))

        for j in range(len(trajectory)):
            state_i = tuple(trajectory[j][:3])
            #print(state_i)
            G_i = sum(rewards[j:])
            if state_i not in updated_return:
                updated_return[state_i] = G_i
                episodic_counts[state_i] = 1
            else:
                updated_return[state_i] += G_i
                episodic_counts[state_i] += 1

        # print("update:", updated_return)
        # print(len(updated_return))

        for z in updated_return.keys():
            if z not in self.counts.keys():
                self.counts[z] = 0

            self.q_table[z] = (self.q_table[z] * self.counts[z] + updated_return[z]) /(self.counts[z] + episodic_counts[z])
            self.counts[z] += episodic_counts[z]

        # print('counts:', self.counts)
        # print(len(self.counts))

        return self.q_table

    def agent_step(self,obs):
        """
        :param obs: list [topic, left_money]
        :return:
        """
        qs = []
        for i in range(len(self.action_space)):
            obs_ai = copy.copy(obs)
            #print(obs_ai)
            ai = self.action_space[i]
            #print(ai, type(ai))
            obs_ai.append(ai)
            #print(obs_ai)
            obs_ai = tuple(obs_ai)
            if obs_ai in self.q_table.keys():
                q_i = self.q_table[obs_ai]
            else:
                q_i = 0

            qs.append(q_i)

        max_q = max(qs)
        max_acts = [idx for idx, act_val in enumerate(qs) if act_val == max_q]

        if random.random() < self.epsilon:
            return random.randint(0, 1)
        else:
            return random.choice(max_acts)


def train(timesteps,budget, episodes): # timesteps = 500, budget = 120
    init = Ready(action_space = [0,1], epsilon= 0.1)

    path = './data_10_topics.csv'
    sandbox = generate_obs(action_space = [0,1], epsilon= 0.1)

    agent = MC_agent(action_space=[0, 1], epsilon=0.1)

    episodes_return = []
    average_returns = []
    episodic_len = []

    tjy_1 = []
    rewards_1 = 0
    money_left = budget
    for t in range(timesteps):
        if money_left > 0:
            obs_t = []
            current_topic, current_click = sandbox.get_samples(path)
            #sample = []
            # sample.append(current_topic)
            # sample.append(current_click)
            # print(sample)
            current_action = init.rdm_pi()
            current_reward, current_cost = sandbox.get_rewards(current_action,current_click)
            obs_t.append(current_topic)
            obs_t.append(money_left)
            obs_t.append(current_action)
            obs_t.append(current_reward)
            rewards_1 += current_reward
            money_left -= current_cost
            tjy_1.append(obs_t)
        else:
            break
    episodes_return.append(rewards_1)
    average_returns.append(rewards_1/len(tjy_1))
    episodic_len.append(len(tjy_1))
    qs_1 = agent.learn_q(tjy_1)

    # print(tjy_1)
    # print(qs_1)
    # print(len(qs_1))

    for i in tqdm(range(episodes-1)):
        tjy_eps = []
        money_left = budget
        rewards_eps = 0

        for t in range(timesteps):
            if money_left > 0:
                obs_epst = []
                epst_topic, epst_click = sandbox.get_samples(path)
                sample = []
                sample.append(epst_topic)
                sample.append(epst_click)
                #print(sample)
                obs_epst.append(epst_topic)
                obs_epst.append(money_left)
                #print(obs_epst)
                epst_action = agent.agent_step(obs_epst)
                epst_reward, epst_cost = sandbox.get_rewards(epst_action, epst_click)
                obs_epst.append(epst_action)
                obs_epst.append(epst_reward)
                money_left -= epst_cost
                rewards_eps += epst_reward
                tjy_eps.append(obs_epst)
            else:
                break

        episodes_return.append(rewards_eps)
        average_returns.append(rewards_eps/len(tjy_eps))
        episodic_len.append(len(tjy_eps))
        qs_eps = agent.learn_q(tjy_eps)

        # print(tjy_eps)
        # print(qs_eps)
        # print(len(qs_eps))

    return episodes_return, average_returns, episodic_len


if __name__ == "__main__":

    random.seed(17)
    episodes_return, average_returns, episodic_len = train(500,120,5000)

    print(episodes_return)
    print(average_returns)
    print(episodic_len)

    plt.figure()
    plt.plot(episodes_return)

    plt.figure()
    plt.plot(average_returns)

    plt.show()









