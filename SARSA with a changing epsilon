import pandas as pd
import numpy as np
from tqdm import tqdm
import random
from collections import defaultdict
import copy
import matplotlib.pyplot as plt

class Ready:
    """
    Get ready and start with first episode by randomly sample data and with random policy
    """
    def __init__(self, action_space: list, epsilon: float, stepsize: float, **kwargs):
        """
        :param action_space:
        :param epsilon:
        :param stepsize:learning rate
        """
        self.action_space = action_space
        self.epsilon = epsilon
        self.stepsize = stepsize

        self.q_table = defaultdict(lambda: 0)

    def rdm_pi(self):
        rdm_action = random.choice(self.action_space)
        return rdm_action


class generate_obs(Ready):
    """
    Generate observations
    """
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def get_samples(self,path):
        sandbox = pd.read_csv(path)
        self.total = len(sandbox)
        idx = random.randint(0,self.total-1)
        self.topics = sandbox['topic_id']
        self.clicks = sandbox['click']
        self.sampled_topic = int(self.topics[idx])
        self.sampled_click = int(self.clicks[idx])

        return self.sampled_topic, self.sampled_click

    def get_rewards(self,action,click):
        if action == self.action_space[1] and click == 1:
            cost = 1.5
            reward = 5
        else:
            reward = 0
            cost = 0
        # if action == self.action_space[1]:
        #     cost = 1.5
        #     if click == 1:
        #         reward = 5
        #     else:
        #         reward = 0

        return reward, cost

class MC_agent(Ready):

    def __init__(self, **kwargs):
        super().__init__(**kwargs)


    def learn_q(self,current_s,reward, current_a, next_s, next_a):
        """
        :param s: list of [topic,left_money]
        :return:
        """
        #print(current_s,reward, current_a, next_s, next_a)
        crt_s = copy.copy(current_s)
        crt_s.append(current_a)
        current_sa = tuple(crt_s)

        nxt_s = copy.copy(next_s)
        nxt_s.append(next_a)
        next_sa = tuple(nxt_s)

        if next_sa in self.q_table.keys():
            q_nxt = self.q_table[next_sa]
        else:
            q_nxt = 0

        self.q_table[current_sa] += self.stepsize * (reward + q_nxt - self.q_table[current_sa])
        #print(current_sa, self.q_table[current_sa])

        return self.q_table
    def normalization(self, m_t):
        """
        normalize m_t to get epsilon
        log-transformation + 0-1 normalization
        :param m_t: money_left / time_left
        :return:
        """
        log_mt = np.log(m_t)
        mt_norm = (log_mt + 1.5404) / (4.7875 + 1.5404)  # max_log_mt,min_log_mt for all ()pairs
        return mt_norm

    def agent_step(self,obs, epsilon):
        """
        :param obs: list [topic, left_money]
        :return:
        """
        qs = []
        for i in range(len(self.action_space)):
            obs_ai = copy.copy(obs)
            #print(obs_ai)
            ai = self.action_space[i]
            #print(ai, type(ai))
            obs_ai.append(ai)
            #print(obs_ai)
            obs_ai = tuple(obs_ai)
            if obs_ai in self.q_table.keys():
                q_i = self.q_table[obs_ai]
            else:
                q_i = 0

            qs.append(q_i)

        max_q = max(qs)
        max_acts = [idx for idx, act_val in enumerate(qs) if act_val == max_q]

        if random.random() < epsilon:
            return random.randint(0, 1)
        else:
            return random.choice(max_acts)


def train(timesteps,budget, episodes): # timesteps = 500, budget = 120
    init = Ready(action_space = [0,1], epsilon= 0.1, stepsize=0.25)

    path = './data_10_topics.csv'
    sandbox = generate_obs(action_space = [0,1], epsilon= 0.1,stepsize=0.25)

    agent = MC_agent(action_space=[0, 1], epsilon=0.1, stepsize=0.25)

    episodes_return = []
    average_returns = []
    episodic_lens = []

    for i in tqdm(range(episodes)):
        tjy_eps = []
        money_left = budget
        rewards_eps = 0

        starting_s = []
        start_topic, start_click = sandbox.get_samples(path)
        starting_s.append(start_topic)
        starting_s.append(money_left)
        current_s = starting_s
        current_click = start_click
        # if i == 0:
        #     current_action = init.rdm_pi()
        # else:
        #     current_action = agent.agent_step(current_s)
        #
        if i == 0:
            current_action = agent.agent_step(current_s, epsilon=0.1)
        else:
            # if i <= 1000:
            #     current_action = agent.agent_step(current_s, epsilon=0.1)
            # else:
            #     m_t = money_left / (500 - 499)
            #     # print(m_t)
            #     eps = agent.normalization(m_t)
            #     current_action = agent.agent_step(current_s, epsilon=eps)
            m_t = money_left / (500 - 499)
            print(m_t)
            eps = agent.normalization(m_t)
            current_action = agent.agent_step(current_s, epsilon=eps)

        for t in range(timesteps):
            if money_left > 0:

                current_reward, current_cost = sandbox.get_rewards(current_action, current_click)
                # if money_left == 120:
                #     realtime_reward = 0
                # else:
                #     realtime_reward = current_reward + (rewards_eps / (120 - money_left))/100
                # rewards_eps += realtime_reward

                next_s = []
                next_topic, next_click = sandbox.get_samples(path)
                money_left -= current_cost
                rewards_eps += current_reward
                next_s.append(next_topic)
                next_s.append(money_left)

                # if i == 0:
                #     next_action = init.rdm_pi()
                # else:
                #     next_action = agent.agent_step(next_s)
                if i == 0:
                    next_action = agent.agent_step(next_s,epsilon=0.1)
                else:
                #     if i <= 1000:
                #         next_action = agent.agent_step(next_s,epsilon=0.1)
                #     else:
                #         m_t = money_left / (500 - t)
                #         # print(m_t)
                #         eps = agent.normalization(m_t)
                #         next_action = agent.agent_step(next_s,epsilon=eps)
                    m_t = money_left / (500 - t)
                    # print(m_t)
                    eps = agent.normalization(m_t)
                    next_action = agent.agent_step(next_s, epsilon=eps)

                if t == timesteps - 1:
                    #print(t)
                    agent.q_table[tuple(current_s)] += agent.stepsize * (current_reward - agent.q_table[tuple(current_s)])
                else:
                    agent.learn_q(current_s,current_reward,current_action,next_s,next_action)

                obs = copy.copy(current_s)
                #print(current_s)
                #print(obs)
                obs.append(current_action)
                obs.append(current_reward)
                tjy_eps.append(obs)

                current_s = next_s
                current_click = next_click
                current_action = next_action

            else:
                break

        episodes_return.append(rewards_eps)
        average_returns.append(rewards_eps/len(tjy_eps))
        episodic_lens.append(len(tjy_eps))

        print(tjy_eps)
        print(agent.q_table)

    return episodes_return, average_returns, episodic_lens


if __name__ == "__main__":

    random.seed(17)
    episodes_return, average_returns, episodic_lens = train(500,120,1000)

    print(episodes_return)
    print(average_returns)
    print(episodic_lens)

    # plt.figure()
    # plt.plot(episodes_return)
    #
    # plt.figure()
    # plt.plot(average_returns)
    #
    # plt.show()









