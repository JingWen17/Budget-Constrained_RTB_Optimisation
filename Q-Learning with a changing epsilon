import pandas as pd
import numpy as np
from tqdm import tqdm
import random
from collections import defaultdict
import copy
import matplotlib.pyplot as plt

class Ready:
    """
    Get ready and start with first episode by randomly sample data
    """
    def __init__(self, action_space: list, epsilon: float, stepsize: float, **kwargs):
        self.action_space = action_space
        self.epsilon = epsilon
        self.stepsize = stepsize

        self.q_table = defaultdict(lambda: 0)



class generate_obs(Ready):
    """
    Generate observations
    """
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def get_samples(self,path):
        sandbox = pd.read_csv(path)
        self.total = len(sandbox)
        idx = random.randint(0,self.total-1)
        self.topics = sandbox['topic_id']
        self.clicks = sandbox['click']
        self.sampled_topic = int(self.topics[idx])
        self.sampled_click = int(self.clicks[idx])

        return self.sampled_topic, self.sampled_click

    def get_rewards(self,action,click):
        if action == self.action_space[1] and click == 1:
            cost = 1.5
            reward = 5
        else:
            reward = 0
            cost = 0

        return reward, cost

class MC_agent(Ready):

    def __init__(self, **kwargs):
        super().__init__(**kwargs)


    def learn_q(self,current_s,reward, current_a, next_s):

        crt_s = copy.copy(current_s)
        crt_s.append(current_a)
        current_sa = tuple(crt_s)

        next_s0 = copy.copy(next_s)
        next_s0.append(0)
        next_s0 = tuple(next_s0)
        next_s1 = copy.copy(next_s)
        next_s1.append(1)
        next_s1 = tuple(next_s1)

        if next_s0 in self.q_table.keys():
            q_s0 = self.q_table[next_s0]
        else:
            q_s0 = 0

        if next_s1 in self.q_table.keys():
            q_s1 = self.q_table[next_s1]
        else:
            q_s1 = 0

        max_q = max(q_s0,q_s1)

        self.q_table[current_sa] += self.stepsize * (reward + max_q - self.q_table[current_sa])

        return self.q_table

    def normalization(self, m_t):
        """
        normalize m_t to get epsilon
        log-transformation + 0-1 normalization
        :param m_t: money_left / time_left
        :return:
        """
        log_mt = np.log(m_t)
        mt_norm = (log_mt + 1.5404) / (4.7875 + 1.5404)  # max_log_mt,min_log_mt for all ()pairs
        return mt_norm

    def agent_step(self,obs, epsilon):

        qs = []
        for i in range(len(self.action_space)):
            obs_ai = copy.copy(obs)
            ai = self.action_space[i]
            obs_ai.append(ai)
            obs_ai = tuple(obs_ai)
            if obs_ai in self.q_table.keys():
                q_i = self.q_table[obs_ai]
            else:
                q_i = 0

            qs.append(q_i)

        max_q = max(qs)
        max_acts = [idx for idx, act_val in enumerate(qs) if act_val == max_q]

        if random.random() < epsilon:
            return random.randint(0, 1)
        else:
            return random.choice(max_acts)


def train(timesteps,budget, episodes): 
    init = Ready(action_space = [0,1], epsilon= 0.1, stepsize=0.25)

    path = './data.csv'
    sandbox = generate_obs(action_space = [0,1], epsilon= 0.1,stepsize=0.25)

    agent = MC_agent(action_space=[0, 1], epsilon=0.1, stepsize=0.25)

    episodes_return = []
    average_returns = []
    episodic_lens = []


    for i in tqdm(range(episodes)):
        tjy_eps = []
        money_left = budget
        rewards_eps = 0

        starting_s = []
        start_topic, start_click = sandbox.get_samples(path)
        starting_s.append(start_topic)
        starting_s.append(money_left)
        current_s = starting_s
        current_click = start_click

        for t in range(timesteps):
            if money_left > 0:
                if i == 0:
                    current_action = agent.agent_step(current_s, epsilon=0.1)
                else:
                    m_t = money_left / (500 - t)
                    eps = agent.normalization(m_t)
                    current_action = agent.agent_step(current_s, epsilon=eps)

                current_reward, current_cost = sandbox.get_rewards(current_action, current_click)

                next_s = []
                next_topic, next_click = sandbox.get_samples(path)
                money_left -= current_cost
                rewards_eps += current_reward
                next_s.append(next_topic)
                next_s.append(money_left)

                if t == timesteps - 1:
                    agent.q_table[tuple(current_s)] += agent.stepsize * (current_reward - agent.q_table[tuple(current_s)])
                else:
                    agent.learn_q(current_s,current_reward,current_action,next_s)

                obs = copy.copy(current_s)
                obs.append(current_action)
                obs.append(current_reward)
                tjy_eps.append(obs)

                current_s = next_s
                current_click = next_click

            else:
                break

        episodes_return.append(rewards_eps)
        average_returns.append(rewards_eps/len(tjy_eps))
        episodic_lens.append(len(tjy_eps))

    return episodes_return, average_returns, episodic_lens


if __name__ == "__main__":

    random.seed(17)
    episodes_return, average_returns, episodic_lens= train(500,120,1000)

    print(episodes_return)
    print(average_returns)
    print(episodic_lens)










